defaults:
  - override hydra/launcher: submitit_slurm

task: mis
input: rb200-300sc
wandb: 0
d: 0  # -1 for cpu
seed: 0
print_freq: 3
wandb_freq: null
eval: false
eval_freq: 200

# for GIN
arch: gin
hidden_dim: 8
hidden_layer: 2
dropout: 0.
aggr: sum
learn_eps: true

# GFlowNet algorithm
alg: fl
onpolicy: true
epochs: 4
trainsize: 100
testsize: 20
tstep: 3
bsit: 2
bs: 16
tbs: 3
shuffle: true # for train loader
num_workers: 0 # for dataloader
sameg: false # use same graph across one batch
tranbuff_size: 1000000  # could be 10,000,000

lr: 1e-3
zlr: 1e-3
randp: 0.
lc: 1 # leaf_coef, for db

# Reward shaping
anneal: linear  # none, linear
annend: 40000
rexp: 5e2 # reward_exp
rexpit: 1
penalty: linear
pargs: [0.75]

# Conditioning
condition: none
project: true
condition_dim: 0 # check hidden dimension of your LLaMA model

# LLM
llm: none
# llm: meta-llama/Llama-3.2-1B-Instruct
llm_batch_size: 8

hydra:
  run:
    dir: ./outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}
  sweep:
    dir: ${oc.env:HOME}/scratch/${now:%Y.%m.%d}/${now:%H.%M.%S}
    subdir: ${hydra.job.override_dirname}

  launcher:
    timeout_min: 1440 # 1 day
    name: ${hydra.job.name}
    partition: long
    mem_gb: 32
    nodes: 1
    gpus_per_node: 1
    cpus_per_task: 1
    tasks_per_node: 1
