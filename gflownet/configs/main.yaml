defaults:
  - override hydra/launcher: submitit_slurm

task: mis
input: rb200-300_5
wandb: 0
device: 0  # -1 for cpu
seed: 0
print_freq: 3
wandb_freq: null
eval: true
eval_freq: 200
eval_model: none

# for GIN
arch: gin
encoding_dim: 16 # positional encoding dimension
hidden_dim: 256
hidden_layer: 5
dropout: 0.
aggr: sum
learn_eps: true

# GFlowNet algorithm
alg: fl
onpolicy: true
epochs: 20
trainsize: 4000
testsize: 500
tstep: 30
batch_size_interact: 8
batch_size: 256
test_batch_size: 30
shuffle: true # for train loader
num_workers: 4 # for dataloader
same_graph_across_batch: false
tranbuff_size: 1000000  # could be 10,000,000

lr: 1e-3
zlr: 1e-3
randp: 0.
leaf_coef: 1 # for db

# Reward shaping
anneal: linear  # none, linear
annend: 40000
reward_exp: 5e2
reward_exp_init: 1
penalty_coef: 0.3

# Conditioning
condition: concat # none, concat, film, (attend)
condition_dim: 3072 # check hidden dimension of your LLaMA model

# LLM
llm: meta-llama/Llama-3.2-3B-Instruct
llm_batch_size: 32

hydra:
  run:
    dir: ./outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}
  sweep:
    dir: ${oc.env:HOME}/scratch/${now:%Y.%m.%d}/${now:%H.%M.%S}
    subdir: ${hydra.job.override_dirname}

  launcher:
    timeout_min: 1440 # 1 day
    name: ${hydra.job.name}
    partition: long
    mem_gb: 32
    nodes: 1
    gpus_per_node: 1
    cpus_per_task: 1
    tasks_per_node: 1
